{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train loader: 17168\n",
      "length of val loader: 308\n"
     ]
    }
   ],
   "source": [
    "from data_utils import generateDataset\n",
    "train_dataloader = generateDataset(dataPath='./dataset/snli_1.0_train.jsonl', isTrain=True)\n",
    "val_dataloader = generateDataset(dataPath='./dataset/snli_1.0_dev.jsonl', isTrain=False)\n",
    "print(\"length of train loader: {}\".format(len(train_dataloader)))\n",
    "print(\"length of val loader: {}\".format(len(val_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 101, 1037, 2158,  ...,    0,    0,    0],\n",
       "         [ 101, 1037, 2158,  ...,    0,    0,    0],\n",
       "         [ 101, 1016, 2308,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1037, 2158,  ...,    0,    0,    0],\n",
       "         [ 101, 1996, 3628,  ...,    0,    0,    0],\n",
       "         [ 101, 2048, 4715,  ...,    0,    0,    0]], dtype=torch.int32),\n",
       " tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]]),\n",
       " tensor([0, 2, 0, 0, 2, 2, 1, 0, 1, 1, 0, 1, 1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2,\n",
       "         0, 0, 0, 1, 1, 1, 0, 1])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "### In this notebook, we used dataset (SNLI) to further validate our conclusion, this is a classification work. It considers the problem on inference, where a pair of sentences (a premise and a hypothesis) are given and we have to classify them as entailment, neutral, negative. If the result is entailment, it means that premise can infer hypothesis. If negative, premise can't infer hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train loader: 17168\n",
      "length of val loader: 308\n"
     ]
    }
   ],
   "source": [
    "from data_utils import generateDataset\n",
    "train_dataloader = generateDataset(dataPath='./dataset/snli_1.0_train.jsonl', isTrain=True)\n",
    "val_dataloader = generateDataset(dataPath='./dataset/snli_1.0_dev.jsonl', isTrain=False)\n",
    "print(\"length of train loader: {}\".format(len(train_dataloader)))\n",
    "print(\"length of val loader: {}\".format(len(val_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from train_epoch import evaluate\n",
    "import torch.nn as nn\n",
    "import time\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# t_begin = time.time()\n",
    "# val_loss, val_acc = evaluate(model, loss_fn, val_dataloader, device, is_pretrained=True)\n",
    "# time_elapsed = time.time() - t_begin\n",
    "# print(f\"{'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "# print(\"-\" * 35)\n",
    "# print(f\"{val_loss:^10.6f} | {val_acc:^9.2f} | {time_elapsed:^9.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   200   |   0.838240   |     -      |     -     |   37.52  \n",
      "   1    |   400   |   0.625384   |     -      |     -     |   35.10  \n",
      "   1    |   600   |   0.589157   |     -      |     -     |   35.23  \n",
      "   1    |   800   |   0.545320   |     -      |     -     |   35.28  \n",
      "   1    |  1000   |   0.539769   |     -      |     -     |   35.35  \n",
      "   1    |  1200   |   0.520024   |     -      |     -     |   35.42  \n",
      "   1    |  1400   |   0.507054   |     -      |     -     |   35.44  \n",
      "   1    |  1600   |   0.487531   |     -      |     -     |   35.43  \n",
      "   1    |  1800   |   0.492652   |     -      |     -     |   35.44  \n",
      "   1    |  2000   |   0.490036   |     -      |     -     |   35.48  \n",
      "   1    |  2200   |   0.456009   |     -      |     -     |   35.51  \n",
      "   1    |  2400   |   0.468178   |     -      |     -     |   35.48  \n",
      "   1    |  2600   |   0.452474   |     -      |     -     |   35.48  \n",
      "   1    |  2800   |   0.469005   |     -      |     -     |   35.49  \n",
      "   1    |  3000   |   0.450209   |     -      |     -     |   35.51  \n",
      "   1    |  3200   |   0.452616   |     -      |     -     |   35.50  \n",
      "   1    |  3400   |   0.423903   |     -      |     -     |   35.49  \n",
      "   1    |  3600   |   0.423390   |     -      |     -     |   35.51  \n",
      "   1    |  3800   |   0.428582   |     -      |     -     |   35.49  \n",
      "   1    |  4000   |   0.422200   |     -      |     -     |   59.35  \n",
      "   1    |  4200   |   0.434323   |     -      |     -     |   35.57  \n",
      "   1    |  4400   |   0.420298   |     -      |     -     |   35.54  \n",
      "   1    |  4600   |   0.401733   |     -      |     -     |   35.51  \n",
      "   1    |  4800   |   0.393757   |     -      |     -     |   35.49  \n",
      "   1    |  5000   |   0.430441   |     -      |     -     |   35.49  \n",
      "   1    |  5200   |   0.411846   |     -      |     -     |   35.46  \n",
      "   1    |  5400   |   0.421631   |     -      |     -     |   35.45  \n",
      "   1    |  5600   |   0.416299   |     -      |     -     |   35.46  \n",
      "   1    |  5800   |   0.398591   |     -      |     -     |   35.46  \n",
      "   1    |  6000   |   0.409217   |     -      |     -     |   35.44  \n",
      "   1    |  6200   |   0.402944   |     -      |     -     |   35.45  \n",
      "   1    |  6400   |   0.403987   |     -      |     -     |   35.44  \n",
      "   1    |  6600   |   0.400834   |     -      |     -     |   35.44  \n",
      "   1    |  6800   |   0.389087   |     -      |     -     |   35.43  \n",
      "   1    |  7000   |   0.385642   |     -      |     -     |   35.42  \n",
      "   1    |  7200   |   0.397114   |     -      |     -     |   35.44  \n",
      "   1    |  7400   |   0.391955   |     -      |     -     |   35.41  \n",
      "   1    |  7600   |   0.388452   |     -      |     -     |   35.43  \n",
      "   1    |  7800   |   0.387317   |     -      |     -     |   35.44  \n",
      "   1    |  8000   |   0.381139   |     -      |     -     |   35.44  \n",
      "   1    |  8200   |   0.407249   |     -      |     -     |   35.45  \n",
      "   1    |  8400   |   0.372341   |     -      |     -     |   35.42  \n",
      "   1    |  8600   |   0.392495   |     -      |     -     |   35.45  \n",
      "   1    |  8800   |   0.374108   |     -      |     -     |   35.42  \n",
      "   1    |  9000   |   0.373582   |     -      |     -     |   35.42  \n",
      "   1    |  9200   |   0.388691   |     -      |     -     |   35.42  \n",
      "   1    |  9400   |   0.376604   |     -      |     -     |   35.42  \n",
      "   1    |  9600   |   0.382007   |     -      |     -     |   35.40  \n",
      "   1    |  9800   |   0.372917   |     -      |     -     |   35.44  \n",
      "   1    |  10000  |   0.372729   |     -      |     -     |   35.46  \n",
      "   1    |  10200  |   0.379849   |     -      |     -     |   35.44  \n",
      "   1    |  10400  |   0.372073   |     -      |     -     |   35.44  \n",
      "   1    |  10600  |   0.366294   |     -      |     -     |   35.41  \n",
      "   1    |  10800  |   0.383451   |     -      |     -     |   35.44  \n",
      "   1    |  11000  |   0.372656   |     -      |     -     |   35.43  \n",
      "   1    |  11200  |   0.377700   |     -      |     -     |   35.43  \n",
      "   1    |  11400  |   0.384121   |     -      |     -     |   35.43  \n",
      "   1    |  11600  |   0.368680   |     -      |     -     |   35.41  \n",
      "   1    |  11800  |   0.359128   |     -      |     -     |   35.43  \n",
      "   1    |  12000  |   0.369648   |     -      |     -     |   35.43  \n",
      "   1    |  12200  |   0.353070   |     -      |     -     |   35.43  \n",
      "   1    |  12400  |   0.373504   |     -      |     -     |   35.43  \n",
      "   1    |  12600  |   0.359396   |     -      |     -     |   35.43  \n",
      "   1    |  12800  |   0.360620   |     -      |     -     |   35.42  \n",
      "   1    |  13000  |   0.365336   |     -      |     -     |   35.43  \n",
      "   1    |  13200  |   0.357509   |     -      |     -     |   35.42  \n",
      "   1    |  13400  |   0.365547   |     -      |     -     |   35.42  \n",
      "   1    |  13600  |   0.338816   |     -      |     -     |   35.44  \n",
      "   1    |  13800  |   0.366971   |     -      |     -     |   35.44  \n",
      "   1    |  14000  |   0.369186   |     -      |     -     |   35.44  \n",
      "   1    |  14200  |   0.370162   |     -      |     -     |   35.42  \n",
      "   1    |  14400  |   0.359937   |     -      |     -     |   35.43  \n",
      "   1    |  14600  |   0.355370   |     -      |     -     |   35.43  \n",
      "   1    |  14800  |   0.345580   |     -      |     -     |   35.43  \n",
      "   1    |  15000  |   0.371317   |     -      |     -     |   35.43  \n",
      "   1    |  15200  |   0.360382   |     -      |     -     |   35.43  \n",
      "   1    |  15400  |   0.345895   |     -      |     -     |   35.41  \n",
      "   1    |  15600  |   0.336540   |     -      |     -     |   35.43  \n",
      "   1    |  15800  |   0.351312   |     -      |     -     |   35.44  \n",
      "   1    |  16000  |   0.352193   |     -      |     -     |   35.43  \n",
      "   1    |  16200  |   0.347347   |     -      |     -     |   35.43  \n",
      "   1    |  16400  |   0.344005   |     -      |     -     |   35.43  \n",
      "   1    |  16600  |   0.333895   |     -      |     -     |   35.41  \n",
      "   1    |  16800  |   0.336956   |     -      |     -     |   35.43  \n",
      "   1    |  17000  |   0.341811   |     -      |     -     |   35.42  \n",
      "   1    |  17167  |   0.342433   |     -      |     -     |   29.58  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.405958   |  0.287443  |   89.29   |  3083.44 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   200   |   0.259519   |     -      |     -     |   35.55  \n",
      "   2    |   400   |   0.251264   |     -      |     -     |   35.42  \n",
      "   2    |   600   |   0.260512   |     -      |     -     |   35.43  \n",
      "   2    |   800   |   0.271032   |     -      |     -     |   35.43  \n",
      "   2    |  1000   |   0.264864   |     -      |     -     |   35.40  \n",
      "   2    |  1200   |   0.264774   |     -      |     -     |   35.41  \n",
      "   2    |  1400   |   0.282191   |     -      |     -     |   35.43  \n",
      "   2    |  1600   |   0.265144   |     -      |     -     |   35.42  \n",
      "   2    |  1800   |   0.260274   |     -      |     -     |   35.43  \n",
      "   2    |  2000   |   0.262168   |     -      |     -     |   35.42  \n",
      "   2    |  2200   |   0.252283   |     -      |     -     |   35.43  \n",
      "   2    |  2400   |   0.269778   |     -      |     -     |   35.41  \n",
      "   2    |  2600   |   0.283895   |     -      |     -     |   35.43  \n",
      "   2    |  2800   |   0.271544   |     -      |     -     |   35.43  \n",
      "   2    |  3000   |   0.282010   |     -      |     -     |   35.44  \n",
      "   2    |  3200   |   0.263988   |     -      |     -     |   35.43  \n",
      "   2    |  3400   |   0.250282   |     -      |     -     |   35.43  \n",
      "   2    |  3600   |   0.252997   |     -      |     -     |   35.44  \n",
      "   2    |  3800   |   0.259165   |     -      |     -     |   35.44  \n",
      "   2    |  4000   |   0.260309   |     -      |     -     |   35.42  \n",
      "   2    |  4200   |   0.268671   |     -      |     -     |   35.40  \n",
      "   2    |  4400   |   0.251879   |     -      |     -     |   35.42  \n",
      "   2    |  4600   |   0.258004   |     -      |     -     |   35.42  \n",
      "   2    |  4800   |   0.265787   |     -      |     -     |   35.41  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |  5000   |   0.259858   |     -      |     -     |   35.44  \n",
      "   2    |  5200   |   0.270202   |     -      |     -     |   35.42  \n",
      "   2    |  5400   |   0.258191   |     -      |     -     |   35.44  \n",
      "   2    |  5600   |   0.253556   |     -      |     -     |   35.42  \n",
      "   2    |  5800   |   0.263675   |     -      |     -     |   35.41  \n",
      "   2    |  6000   |   0.251296   |     -      |     -     |   35.46  \n",
      "   2    |  6200   |   0.255036   |     -      |     -     |   35.43  \n",
      "   2    |  6400   |   0.255753   |     -      |     -     |   35.43  \n",
      "   2    |  6600   |   0.265170   |     -      |     -     |   35.44  \n",
      "   2    |  6800   |   0.263720   |     -      |     -     |   35.44  \n",
      "   2    |  7000   |   0.262774   |     -      |     -     |   35.43  \n",
      "   2    |  7200   |   0.260200   |     -      |     -     |   35.43  \n",
      "   2    |  7400   |   0.257238   |     -      |     -     |   35.45  \n",
      "   2    |  7600   |   0.271540   |     -      |     -     |   35.44  \n",
      "   2    |  7800   |   0.267778   |     -      |     -     |   35.46  \n",
      "   2    |  8000   |   0.261495   |     -      |     -     |   35.43  \n",
      "   2    |  8200   |   0.256292   |     -      |     -     |   35.43  \n",
      "   2    |  8400   |   0.252789   |     -      |     -     |   35.42  \n",
      "   2    |  8600   |   0.266258   |     -      |     -     |   35.43  \n",
      "   2    |  8800   |   0.251621   |     -      |     -     |   35.44  \n",
      "   2    |  9000   |   0.250932   |     -      |     -     |   35.45  \n",
      "   2    |  9200   |   0.248172   |     -      |     -     |   35.43  \n",
      "   2    |  9400   |   0.254185   |     -      |     -     |   35.43  \n",
      "   2    |  9600   |   0.260046   |     -      |     -     |   35.45  \n",
      "   2    |  9800   |   0.261530   |     -      |     -     |   35.44  \n",
      "   2    |  10000  |   0.260485   |     -      |     -     |   35.45  \n",
      "   2    |  10200  |   0.254004   |     -      |     -     |   35.44  \n",
      "   2    |  10400  |   0.246188   |     -      |     -     |   35.42  \n",
      "   2    |  10600  |   0.257570   |     -      |     -     |   35.41  \n",
      "   2    |  10800  |   0.240750   |     -      |     -     |   35.41  \n",
      "   2    |  11000  |   0.252374   |     -      |     -     |   35.43  \n",
      "   2    |  11200  |   0.252607   |     -      |     -     |   35.44  \n",
      "   2    |  11400  |   0.240524   |     -      |     -     |   35.41  \n",
      "   2    |  11600  |   0.237666   |     -      |     -     |   35.43  \n",
      "   2    |  11800  |   0.240513   |     -      |     -     |   35.42  \n",
      "   2    |  12000  |   0.247973   |     -      |     -     |   35.41  \n",
      "   2    |  12200  |   0.250516   |     -      |     -     |   35.43  \n",
      "   2    |  12400  |   0.254168   |     -      |     -     |   35.40  \n",
      "   2    |  12600  |   0.243301   |     -      |     -     |   35.44  \n",
      "   2    |  12800  |   0.259775   |     -      |     -     |   35.43  \n",
      "   2    |  13000  |   0.244180   |     -      |     -     |   35.43  \n",
      "   2    |  13200  |   0.227499   |     -      |     -     |   35.43  \n",
      "   2    |  13400  |   0.254520   |     -      |     -     |   35.43  \n",
      "   2    |  13600  |   0.241946   |     -      |     -     |   35.41  \n",
      "   2    |  13800  |   0.249283   |     -      |     -     |   35.43  \n",
      "   2    |  14000  |   0.245728   |     -      |     -     |   35.42  \n",
      "   2    |  14200  |   0.237140   |     -      |     -     |   35.41  \n",
      "   2    |  14400  |   0.241010   |     -      |     -     |   35.41  \n",
      "   2    |  14600  |   0.244246   |     -      |     -     |   35.44  \n",
      "   2    |  14800  |   0.239690   |     -      |     -     |   35.45  \n",
      "   2    |  15000  |   0.259598   |     -      |     -     |   35.42  \n",
      "   2    |  15200  |   0.241743   |     -      |     -     |   35.41  \n",
      "   2    |  15400  |   0.236616   |     -      |     -     |   35.44  \n",
      "   2    |  15600  |   0.248292   |     -      |     -     |   35.45  \n",
      "   2    |  15800  |   0.241529   |     -      |     -     |   35.44  \n",
      "   2    |  16000  |   0.241970   |     -      |     -     |   35.44  \n",
      "   2    |  16200  |   0.231925   |     -      |     -     |   35.44  \n",
      "   2    |  16400  |   0.247009   |     -      |     -     |   35.41  \n",
      "   2    |  16600  |   0.245440   |     -      |     -     |   35.42  \n",
      "   2    |  16800  |   0.243481   |     -      |     -     |   35.44  \n",
      "   2    |  17000  |   0.250669   |     -      |     -     |   35.42  \n",
      "   2    |  17167  |   0.230872   |     -      |     -     |   29.58  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.254632   |  0.280079  |   90.47   |  3056.97 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from train_epoch import train, evaluate, bert_predict\n",
    "\n",
    "epochs = 2\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "train_losses, val_losses, val_acc = train(model, loss_fn, optimizer, scheduler, train_dataloader, val_dataloader, epochs=epochs, evaluation=True, device=device, FINETUNE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"train loss\", c='b')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(val_losses, label=\"val loss\", c='y')\n",
    "plt.plot(val_acc, label=\"accuracy of val data\", c='g')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Model (Finetune BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "def bertPredict(model, b_input_ids, b_attn_mask, FINETUNE):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, b_attn_mask)\n",
    "        if FINETUNE:\n",
    "            logits = logits[\"logits\"]\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    predictions = []\n",
    "    for pred in preds.cpu().numpy():\n",
    "        predictions.append(label[pred])\n",
    "    return predictions\n",
    "\n",
    "def evaluateRandomly(model, test_dataloader, n=1, FINETUNE=False):\n",
    "    for i, sample in enumerate(test_dataloader):\n",
    "        if i > n: break\n",
    "            \n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in sample)\n",
    "        predict = bertPredict(model, b_input_ids, b_attn_mask, FINETUNE)\n",
    "        b_labels = b_labels.cpu().numpy()\n",
    "        for i, pred in enumerate(b_labels):\n",
    "            print(\"label: {} --- predict: {}\".format(label[b_labels[i]], predict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 90.45195439739413\n",
      "label: neutral --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: contradiction\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: entailment --- predict: entailment\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: neutral\n",
      "label: neutral --- predict: neutral\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: neutral --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n"
     ]
    }
   ],
   "source": [
    "from train_epoch import evaluate\n",
    "\n",
    "test_dataloader = generateDataset(dataPath='./dataset/snli_1.0_test.jsonl', isTrain=False)\n",
    "test_loss, test_acc = evaluate(model, loss_fn, test_dataloader, device=device, FINETUNE=True)\n",
    "print(\"Accuracy on test dataset: {}\".format(test_acc))\n",
    "evaluateRandomly(model, test_dataloader, FINETUNE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Layer 12 in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from load_model import BertClassifier\n",
    "from train_epoch import train, evaluate, bert_predict\n",
    "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertClassifier(freeze_bert=False)\n",
    "print(model)\n",
    "model.to(device)\n",
    "epochs = 2\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   200   |   0.809586   |     -      |     -     |   32.63  \n",
      "   1    |   400   |   0.609162   |     -      |     -     |   32.58  \n",
      "   1    |   600   |   0.570329   |     -      |     -     |   32.57  \n",
      "   1    |   800   |   0.544479   |     -      |     -     |   32.58  \n",
      "   1    |  1000   |   0.534167   |     -      |     -     |   32.58  \n",
      "   1    |  1200   |   0.508499   |     -      |     -     |   32.58  \n",
      "   1    |  1400   |   0.501441   |     -      |     -     |   32.59  \n",
      "   1    |  1600   |   0.478903   |     -      |     -     |   32.60  \n",
      "   1    |  1800   |   0.461566   |     -      |     -     |   32.61  \n",
      "   1    |  2000   |   0.474568   |     -      |     -     |   32.57  \n",
      "   1    |  2200   |   0.451919   |     -      |     -     |   32.59  \n",
      "   1    |  2400   |   0.444115   |     -      |     -     |   32.60  \n",
      "   1    |  2600   |   0.477286   |     -      |     -     |   32.61  \n",
      "   1    |  2800   |   0.439669   |     -      |     -     |   32.59  \n",
      "   1    |  3000   |   0.447899   |     -      |     -     |   32.59  \n",
      "   1    |  3200   |   0.441193   |     -      |     -     |   32.58  \n",
      "   1    |  3400   |   0.433670   |     -      |     -     |   32.58  \n",
      "   1    |  3600   |   0.432110   |     -      |     -     |   32.59  \n",
      "   1    |  3800   |   0.436605   |     -      |     -     |   32.58  \n",
      "   1    |  4000   |   0.438141   |     -      |     -     |   32.58  \n",
      "   1    |  4200   |   0.426033   |     -      |     -     |   32.56  \n",
      "   1    |  4400   |   0.430956   |     -      |     -     |   32.59  \n",
      "   1    |  4600   |   0.423457   |     -      |     -     |   32.60  \n",
      "   1    |  4800   |   0.405261   |     -      |     -     |   32.60  \n",
      "   1    |  5000   |   0.405348   |     -      |     -     |   32.59  \n",
      "   1    |  5200   |   0.408353   |     -      |     -     |   32.59  \n",
      "   1    |  5400   |   0.406672   |     -      |     -     |   32.62  \n",
      "   1    |  5600   |   0.410974   |     -      |     -     |   32.58  \n",
      "   1    |  5800   |   0.409399   |     -      |     -     |   32.56  \n",
      "   1    |  6000   |   0.415258   |     -      |     -     |   32.57  \n",
      "   1    |  6200   |   0.403527   |     -      |     -     |   32.59  \n",
      "   1    |  6400   |   0.403724   |     -      |     -     |   32.60  \n",
      "   1    |  6600   |   0.396286   |     -      |     -     |   32.57  \n",
      "   1    |  6800   |   0.373392   |     -      |     -     |   32.58  \n",
      "   1    |  7000   |   0.400572   |     -      |     -     |   32.58  \n",
      "   1    |  7200   |   0.392777   |     -      |     -     |   32.60  \n",
      "   1    |  7400   |   0.390687   |     -      |     -     |   32.60  \n",
      "   1    |  7600   |   0.386270   |     -      |     -     |   32.58  \n",
      "   1    |  7800   |   0.373916   |     -      |     -     |   32.59  \n",
      "   1    |  8000   |   0.388043   |     -      |     -     |   32.58  \n",
      "   1    |  8200   |   0.378288   |     -      |     -     |   32.59  \n",
      "   1    |  8400   |   0.390546   |     -      |     -     |   32.58  \n",
      "   1    |  8600   |   0.392715   |     -      |     -     |   32.59  \n",
      "   1    |  8800   |   0.363348   |     -      |     -     |   32.60  \n",
      "   1    |  9000   |   0.387586   |     -      |     -     |   32.59  \n",
      "   1    |  9200   |   0.380008   |     -      |     -     |   32.58  \n",
      "   1    |  9400   |   0.378754   |     -      |     -     |   32.60  \n",
      "   1    |  9600   |   0.383012   |     -      |     -     |   32.60  \n",
      "   1    |  9800   |   0.361589   |     -      |     -     |   32.59  \n",
      "   1    |  10000  |   0.384017   |     -      |     -     |   32.59  \n",
      "   1    |  10200  |   0.375951   |     -      |     -     |   32.60  \n",
      "   1    |  10400  |   0.370087   |     -      |     -     |   32.58  \n",
      "   1    |  10600  |   0.372691   |     -      |     -     |   32.60  \n",
      "   1    |  10800  |   0.354048   |     -      |     -     |   32.60  \n",
      "   1    |  11000  |   0.367452   |     -      |     -     |   32.59  \n",
      "   1    |  11200  |   0.370854   |     -      |     -     |   32.61  \n",
      "   1    |  11400  |   0.384430   |     -      |     -     |   32.58  \n",
      "   1    |  11600  |   0.364576   |     -      |     -     |   32.57  \n",
      "   1    |  11800  |   0.370910   |     -      |     -     |   32.57  \n",
      "   1    |  12000  |   0.375405   |     -      |     -     |   32.59  \n",
      "   1    |  12200  |   0.362587   |     -      |     -     |   32.58  \n",
      "   1    |  12400  |   0.360651   |     -      |     -     |   32.58  \n",
      "   1    |  12600  |   0.367395   |     -      |     -     |   32.58  \n",
      "   1    |  12800  |   0.358266   |     -      |     -     |   32.58  \n",
      "   1    |  13000  |   0.351537   |     -      |     -     |   32.58  \n",
      "   1    |  13200  |   0.359627   |     -      |     -     |   32.58  \n",
      "   1    |  13400  |   0.351134   |     -      |     -     |   32.58  \n",
      "   1    |  13600  |   0.341302   |     -      |     -     |   32.57  \n",
      "   1    |  13800  |   0.353716   |     -      |     -     |   32.59  \n",
      "   1    |  14000  |   0.359815   |     -      |     -     |   32.58  \n",
      "   1    |  14200  |   0.353867   |     -      |     -     |   32.57  \n",
      "   1    |  14400  |   0.345420   |     -      |     -     |   32.60  \n",
      "   1    |  14600  |   0.354883   |     -      |     -     |   32.59  \n",
      "   1    |  14800  |   0.341795   |     -      |     -     |   32.59  \n",
      "   1    |  15000  |   0.358228   |     -      |     -     |   32.58  \n",
      "   1    |  15200  |   0.350381   |     -      |     -     |   32.59  \n",
      "   1    |  15400  |   0.354543   |     -      |     -     |   32.59  \n",
      "   1    |  15600  |   0.353408   |     -      |     -     |   32.60  \n",
      "   1    |  15800  |   0.345753   |     -      |     -     |   32.60  \n",
      "   1    |  16000  |   0.345881   |     -      |     -     |   32.60  \n",
      "   1    |  16200  |   0.344984   |     -      |     -     |   32.61  \n",
      "   1    |  16400  |   0.349886   |     -      |     -     |   32.57  \n",
      "   1    |  16600  |   0.363115   |     -      |     -     |   32.59  \n",
      "   1    |  16800  |   0.351674   |     -      |     -     |   32.60  \n",
      "   1    |  17000  |   0.329188   |     -      |     -     |   32.61  \n",
      "   1    |  17167  |   0.362240   |     -      |     -     |   27.21  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.403169   |  0.298686  |   89.00   |  2811.80 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   200   |   0.265470   |     -      |     -     |   32.69  \n",
      "   2    |   400   |   0.273672   |     -      |     -     |   32.61  \n",
      "   2    |   600   |   0.270551   |     -      |     -     |   32.58  \n",
      "   2    |   800   |   0.279254   |     -      |     -     |   32.62  \n",
      "   2    |  1000   |   0.260561   |     -      |     -     |   32.60  \n",
      "   2    |  1200   |   0.268541   |     -      |     -     |   32.65  \n",
      "   2    |  1400   |   0.250664   |     -      |     -     |   32.60  \n",
      "   2    |  1600   |   0.248022   |     -      |     -     |   32.60  \n",
      "   2    |  1800   |   0.250651   |     -      |     -     |   32.60  \n",
      "   2    |  2000   |   0.260003   |     -      |     -     |   32.58  \n",
      "   2    |  2200   |   0.267832   |     -      |     -     |   32.56  \n",
      "   2    |  2400   |   0.259580   |     -      |     -     |   32.58  \n",
      "   2    |  2600   |   0.274125   |     -      |     -     |   32.57  \n",
      "   2    |  2800   |   0.263519   |     -      |     -     |   32.58  \n",
      "   2    |  3000   |   0.256259   |     -      |     -     |   32.59  \n",
      "   2    |  3200   |   0.273060   |     -      |     -     |   32.59  \n",
      "   2    |  3400   |   0.267328   |     -      |     -     |   32.60  \n",
      "   2    |  3600   |   0.253855   |     -      |     -     |   32.59  \n",
      "   2    |  3800   |   0.264522   |     -      |     -     |   32.58  \n",
      "   2    |  4000   |   0.245647   |     -      |     -     |   32.60  \n",
      "   2    |  4200   |   0.270017   |     -      |     -     |   32.60  \n",
      "   2    |  4400   |   0.254087   |     -      |     -     |   32.61  \n",
      "   2    |  4600   |   0.234783   |     -      |     -     |   32.60  \n",
      "   2    |  4800   |   0.262598   |     -      |     -     |   32.57  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |  5000   |   0.260939   |     -      |     -     |   32.60  \n",
      "   2    |  5200   |   0.262198   |     -      |     -     |   32.59  \n",
      "   2    |  5400   |   0.270749   |     -      |     -     |   32.60  \n",
      "   2    |  5600   |   0.264257   |     -      |     -     |   32.60  \n",
      "   2    |  5800   |   0.255035   |     -      |     -     |   32.58  \n",
      "   2    |  6000   |   0.252058   |     -      |     -     |   32.57  \n",
      "   2    |  6200   |   0.260465   |     -      |     -     |   32.56  \n",
      "   2    |  6400   |   0.259680   |     -      |     -     |   32.57  \n",
      "   2    |  6600   |   0.271293   |     -      |     -     |   32.58  \n",
      "   2    |  6800   |   0.252020   |     -      |     -     |   32.60  \n",
      "   2    |  7000   |   0.264054   |     -      |     -     |   32.58  \n",
      "   2    |  7200   |   0.249459   |     -      |     -     |   32.59  \n",
      "   2    |  7400   |   0.239247   |     -      |     -     |   32.59  \n",
      "   2    |  7600   |   0.249089   |     -      |     -     |   32.59  \n",
      "   2    |  7800   |   0.261365   |     -      |     -     |   32.58  \n",
      "   2    |  8000   |   0.253314   |     -      |     -     |   32.57  \n",
      "   2    |  8200   |   0.267029   |     -      |     -     |   32.55  \n",
      "   2    |  8400   |   0.261945   |     -      |     -     |   32.59  \n",
      "   2    |  8600   |   0.275172   |     -      |     -     |   32.59  \n",
      "   2    |  8800   |   0.260259   |     -      |     -     |   32.56  \n",
      "   2    |  9000   |   0.252716   |     -      |     -     |   32.60  \n",
      "   2    |  9200   |   0.243532   |     -      |     -     |   32.58  \n",
      "   2    |  9400   |   0.252370   |     -      |     -     |   32.60  \n",
      "   2    |  9600   |   0.254488   |     -      |     -     |   32.56  \n",
      "   2    |  9800   |   0.258477   |     -      |     -     |   32.59  \n",
      "   2    |  10000  |   0.263524   |     -      |     -     |   32.58  \n",
      "   2    |  10200  |   0.245081   |     -      |     -     |   32.58  \n",
      "   2    |  10400  |   0.252332   |     -      |     -     |   32.59  \n",
      "   2    |  10600  |   0.246600   |     -      |     -     |   32.58  \n",
      "   2    |  10800  |   0.247893   |     -      |     -     |   32.59  \n",
      "   2    |  11000  |   0.261101   |     -      |     -     |   32.58  \n",
      "   2    |  11200  |   0.257599   |     -      |     -     |   32.58  \n",
      "   2    |  11400  |   0.243947   |     -      |     -     |   32.57  \n",
      "   2    |  11600  |   0.240665   |     -      |     -     |   32.59  \n",
      "   2    |  11800  |   0.248306   |     -      |     -     |   32.61  \n",
      "   2    |  12000  |   0.264011   |     -      |     -     |   32.59  \n",
      "   2    |  12200  |   0.247985   |     -      |     -     |   32.61  \n",
      "   2    |  12400  |   0.247395   |     -      |     -     |   32.59  \n",
      "   2    |  12600  |   0.238161   |     -      |     -     |   32.61  \n",
      "   2    |  12800  |   0.256005   |     -      |     -     |   32.59  \n",
      "   2    |  13000  |   0.255252   |     -      |     -     |   32.61  \n",
      "   2    |  13200  |   0.246558   |     -      |     -     |   32.60  \n",
      "   2    |  13400  |   0.260145   |     -      |     -     |   32.58  \n",
      "   2    |  13600  |   0.257404   |     -      |     -     |   32.55  \n",
      "   2    |  13800  |   0.260431   |     -      |     -     |   32.58  \n",
      "   2    |  14000  |   0.242753   |     -      |     -     |   32.59  \n",
      "   2    |  14200  |   0.229613   |     -      |     -     |   32.58  \n",
      "   2    |  14400  |   0.225344   |     -      |     -     |   32.58  \n",
      "   2    |  14600  |   0.240876   |     -      |     -     |   32.60  \n",
      "   2    |  14800  |   0.243454   |     -      |     -     |   32.61  \n",
      "   2    |  15000  |   0.244631   |     -      |     -     |   32.61  \n",
      "   2    |  15200  |   0.245949   |     -      |     -     |   32.60  \n",
      "   2    |  15400  |   0.250637   |     -      |     -     |   32.57  \n",
      "   2    |  15600  |   0.244029   |     -      |     -     |   32.58  \n",
      "   2    |  15800  |   0.240381   |     -      |     -     |   32.59  \n",
      "   2    |  16000  |   0.236738   |     -      |     -     |   32.59  \n",
      "   2    |  16200  |   0.246413   |     -      |     -     |   32.60  \n",
      "   2    |  16400  |   0.239746   |     -      |     -     |   32.58  \n",
      "   2    |  16600  |   0.239305   |     -      |     -     |   32.57  \n",
      "   2    |  16800  |   0.244500   |     -      |     -     |   32.56  \n",
      "   2    |  17000  |   0.235734   |     -      |     -     |   32.57  \n",
      "   2    |  17167  |   0.244812   |     -      |     -     |   27.18  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.254218   |  0.279590  |   90.80   |  2811.76 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "train_losses, val_losses, val_acc = train(model, loss_fn, optimizer, scheduler, train_dataloader, val_dataloader, epochs=epochs, evaluation=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"train loss\", c='b')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(val_losses, label=\"val loss\", c='y')\n",
    "plt.plot(val_acc, label=\"accuracy of val data\", c='g')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Model (remove layer 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "def bertPredict(model, b_input_ids, b_attn_mask, FINETUNE):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, b_attn_mask)\n",
    "        if FINETUNE:\n",
    "            logits = logits[\"logits\"]\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    predictions = []\n",
    "    for pred in preds.cpu().numpy():\n",
    "        predictions.append(label[pred])\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluateRandomly(model, test_dataloader, n=1, FINETUNE=False):\n",
    "    for i, sample in enumerate(test_dataloader):\n",
    "        if i > n: break\n",
    "            \n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in sample)\n",
    "        predict = bertPredict(model, b_input_ids, b_attn_mask, FINETUNE)\n",
    "        b_labels = b_labels.cpu().numpy()\n",
    "        for i, pred in enumerate(b_labels):\n",
    "            print(\"label: {} --- predict: {}\".format(label[b_labels[i]], predict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 90.32980456026058\n",
      "label: neutral --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: entailment --- predict: entailment\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: contradiction\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: neutral\n",
      "label: neutral --- predict: neutral\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: contradiction --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: entailment --- predict: entailment\n",
      "label: contradiction --- predict: contradiction\n",
      "label: neutral --- predict: neutral\n",
      "label: neutral --- predict: contradiction\n",
      "label: entailment --- predict: entailment\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = generateDataset(dataPath='./dataset/snli_1.0_test.jsonl', isTrain=False)\n",
    "test_loss, test_acc = evaluate(model, loss_fn, test_dataloader, device=device, FINETUNE=False)\n",
    "print(\"Accuracy on test dataset: {}\".format(test_acc))\n",
    "evaluateRandomly(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
